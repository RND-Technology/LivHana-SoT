#!/usr/bin/env node
require('dotenv').config({ path: '../.env' });
const { BigQuery } = require('@google-cloud/bigquery');

const PROJECT_ID = process.env.GCP_PROJECT_ID;
const DATASET = process.env.BQ_DATASET || 'commerce';
const LOCATION = process.env.BQ_LOCATION || 'US';

const bigquery = new BigQuery({ projectId: PROJECT_ID });

console.log('üöÄ BIGQUERY PARTITIONING MIGRATION');
console.log('===================================');
console.log(`Project: ${PROJECT_ID}`);
console.log(`Dataset: ${DATASET}`);
console.log(`Location: ${LOCATION}\n`);

const PARTITIONED_SCHEMA = {
  square_payments_partitioned: {
    fields: [
      { name: 'id', type: 'STRING', mode: 'REQUIRED' },
      { name: 'amount', type: 'INTEGER', mode: 'REQUIRED' },
      { name: 'currency', type: 'STRING', mode: 'REQUIRED' },
      { name: 'status', type: 'STRING', mode: 'REQUIRED' },
      { name: 'customer_id', type: 'STRING', mode: 'NULLABLE' },
      { name: 'location_id', type: 'STRING', mode: 'NULLABLE' },
      { name: 'created_at', type: 'TIMESTAMP', mode: 'REQUIRED' },
      { name: 'updated_at', type: 'TIMESTAMP', mode: 'NULLABLE' },
      { name: 'source_type', type: 'STRING', mode: 'NULLABLE' },
      { name: 'card_brand', type: 'STRING', mode: 'NULLABLE' },
      { name: 'receipt_url', type: 'STRING', mode: 'NULLABLE' }
    ],
    timePartitioning: {
      type: 'DAY',
      field: 'created_at',
      expirationMs: null // No auto-expiration
    },
    clustering: {
      fields: ['customer_id', 'status']
    }
  },
  square_items_partitioned: {
    fields: [
      { name: 'id', type: 'STRING', mode: 'REQUIRED' },
      { name: 'name', type: 'STRING', mode: 'REQUIRED' },
      { name: 'category', type: 'STRING', mode: 'NULLABLE' },
      { name: 'sku', type: 'STRING', mode: 'NULLABLE' },
      { name: 'price', type: 'INTEGER', mode: 'NULLABLE' },
      { name: 'available', type: 'BOOLEAN', mode: 'NULLABLE' },
      { name: 'created_at', type: 'TIMESTAMP', mode: 'REQUIRED' },
      { name: 'updated_at', type: 'TIMESTAMP', mode: 'NULLABLE' }
    ],
    timePartitioning: {
      type: 'DAY',
      field: 'updated_at'
    },
    clustering: {
      fields: ['category', 'available']
    }
  }
};

async function createPartitionedTable(tableName, config) {
  console.log(`\nüìä Creating partitioned table: ${tableName}`);

  const table = bigquery.dataset(DATASET).table(tableName);
  const [exists] = await table.exists();

  if (exists) {
    console.log(`‚ö†Ô∏è  Table ${tableName} already exists. Skipping...`);
    return;
  }

  try {
    await table.create({
      schema: { fields: config.fields },
      timePartitioning: config.timePartitioning,
      clustering: config.clustering,
      location: LOCATION
    });
    console.log(`‚úÖ Created partitioned table: ${tableName}`);
    console.log(`   - Partitioned by: ${config.timePartitioning.field} (${config.timePartitioning.type})`);
    console.log(`   - Clustered by: ${config.clustering.fields.join(', ')}`);
  } catch (error) {
    console.error(`‚ùå Failed to create ${tableName}:`, error.message);
    throw error;
  }
}

async function copyDataToPartitioned(sourceTable, destTable) {
  console.log(`\nüì¶ Copying data: ${sourceTable} ‚Üí ${destTable}`);

  const source = bigquery.dataset(DATASET).table(sourceTable);
  const [sourceExists] = await source.exists();

  if (!sourceExists) {
    console.log(`‚ö†Ô∏è  Source table ${sourceTable} does not exist. Skipping copy...`);
    return;
  }

  const query = `
    INSERT INTO \`${PROJECT_ID}.${DATASET}.${destTable}\`
    SELECT * FROM \`${PROJECT_ID}.${DATASET}.${sourceTable}\`
  `;

  console.log('   Running copy job...');
  const startTime = Date.now();

  try {
    const [job] = await bigquery.createQueryJob({
      query,
      location: LOCATION
    });

    await job.getQueryResults();
    const duration = Date.now() - startTime;

    console.log(`‚úÖ Data copied in ${duration}ms`);

    // Get row count
    const countQuery = `SELECT COUNT(*) as count FROM \`${PROJECT_ID}.${DATASET}.${destTable}\``;
    const [countResult] = await bigquery.query({ query: countQuery, location: LOCATION });
    console.log(`   Total rows: ${countResult[0].count}`);
  } catch (error) {
    console.error(`‚ùå Failed to copy data:`, error.message);
    throw error;
  }
}

async function updateTableAlias(oldTable, newTable) {
  console.log(`\nüîÑ Creating view alias: ${oldTable} ‚Üí ${newTable}`);

  const viewQuery = `
    CREATE OR REPLACE VIEW \`${PROJECT_ID}.${DATASET}.${oldTable}\` AS
    SELECT * FROM \`${PROJECT_ID}.${DATASET}.${newTable}\`
  `;

  try {
    await bigquery.query({ query: viewQuery, location: LOCATION });
    console.log(`‚úÖ View created: ${oldTable} now points to ${newTable}`);
  } catch (error) {
    console.error(`‚ùå Failed to create view:`, error.message);
    throw error;
  }
}

async function analyzeTables() {
  console.log('\nüìà PERFORMANCE ANALYSIS');
  console.log('======================\n');

  for (const tableName of Object.keys(PARTITIONED_SCHEMA)) {
    const table = bigquery.dataset(DATASET).table(tableName);
    const [exists] = await table.exists();

    if (!exists) continue;

    console.log(`üìä ${tableName}:`);

    try {
      const [metadata] = await table.getMetadata();
      const numBytes = parseInt(metadata.numBytes || 0);
      const numRows = parseInt(metadata.numRows || 0);

      console.log(`   Rows: ${numRows.toLocaleString()}`);
      console.log(`   Size: ${(numBytes / 1024 / 1024).toFixed(2)} MB`);
      console.log(`   Partitions: ${metadata.timePartitioning ? 'Enabled' : 'None'}`);
      console.log(`   Clustering: ${metadata.clustering ? metadata.clustering.fields.join(', ') : 'None'}`);

      // Estimate query cost reduction
      const originalScan = numBytes;
      const partitionedScan = numBytes / 180; // Assuming 180-day partition window
      const savings = ((originalScan - partitionedScan) / originalScan * 100).toFixed(1);

      console.log(`   Est. scan reduction: ${savings}% (for 1-day queries)`);
    } catch (error) {
      console.error(`   ‚ùå Failed to get metadata: ${error.message}`);
    }
    console.log('');
  }
}

async function runMigration() {
  console.log('STEP 1: Create partitioned tables\n');

  for (const [tableName, config] of Object.entries(PARTITIONED_SCHEMA)) {
    await createPartitionedTable(tableName, config);
  }

  console.log('\n\nSTEP 2: Copy data to partitioned tables\n');

  await copyDataToPartitioned('square_payments', 'square_payments_partitioned');
  await copyDataToPartitioned('square_items', 'square_items_partitioned');

  console.log('\n\nSTEP 3: Performance analysis\n');

  await analyzeTables();

  console.log('\nüéâ MIGRATION COMPLETE!\n');
  console.log('NEXT STEPS:');
  console.log('1. Update .env to use partitioned tables:');
  console.log('   BQ_TABLE_PAYMENTS=square_payments_partitioned');
  console.log('   BQ_TABLE_ITEMS=square_items_partitioned');
  console.log('');
  console.log('2. Test queries with new tables');
  console.log('3. Monitor performance improvements');
  console.log('4. Once validated, optionally drop old tables\n');
  console.log('üìä Expected improvements:');
  console.log('   - Query latency: 80% reduction (2-5s ‚Üí 200-400ms)');
  console.log('   - Data scanned: 99% reduction (180 days ‚Üí 1 day scans)');
  console.log('   - Cost: ~99% reduction per query');
  console.log('');
}

runMigration().catch((error) => {
  console.error('\n‚ùå MIGRATION FAILED:', error);
  process.exit(1);
});
// Last optimized: 2025-10-02
