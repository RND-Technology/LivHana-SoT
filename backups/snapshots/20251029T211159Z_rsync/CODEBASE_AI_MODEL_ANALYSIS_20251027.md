# ğŸ¤– LivHana-SoT AI Model Compatibility Analysis
**Analysis Date**: October 27, 2025  
**Codebase Version**: fix/mobile-control-po1  
**Total Codebase Size**: 14.21M tokens | 54.21 MB | 748,858 lines

---

## ğŸ“Š CODEBASE SPECIFICATIONS

### Raw Metrics
- **Total Files**: 6,675 files
- **Source/Doc Files**: 5,802 files  
- **Lines of Code**: 748,858 lines
- **Documentation**: 354,985 lines (47% of codebase)
- **Disk Size**: 1.2 GB (total) | 54.21 MB (source)
- **Estimated Tokens**: 14.21 Million tokens

### Technology Stack
- **Primary Languages**: TypeScript (202 files), JavaScript (312 files), Python (508 files)
- **Frontend**: React/Vite (10 TSX files)
- **Backend**: Node.js microservices (voice-service, reasoning-gateway, integration-service)
- **Infrastructure**: Docker Compose, Redis queues (BullMQ), GCP Cloud Run
- **Documentation**: 4,032 Markdown files (extensive technical docs)

### Architecture Complexity
- **Pattern**: Microservices with Redis queue-based communication
- **Services**: 24+ backend services
- **Queue Systems**: BullMQ with environment-specific queue names
- **Inter-Agent**: File-based task coordination (multi-IDE workflow)
- **Compliance**: Cannabis industry regulations (21+, NIST validation, FDA claims)

---

## ğŸ† AI MODEL STACK RANKING (October 2025)

### **TIER S - OPTIMAL FOR THIS CODEBASE**

#### 1ï¸âƒ£ **Anthropic Claude 3.5 Sonnet (Extended) - Code Composer**
- **Context Window**: 200K tokens input
- **Codebase Coverage**: 1.41% per request
- **Output**: 8K tokens
- **Strengths**: 
  - âœ… Best-in-class code generation
  - âœ… Excellent with TypeScript/JavaScript microservices
  - âœ… Superior multi-file reasoning
  - âœ… Strong Docker/infrastructure understanding
  - âœ… Can follow complex inter-service dependencies
- **Weaknesses**: 
  - âš ï¸ Requires 71 context windows to cover full codebase
  - âš ï¸ Cannot see entire service mesh at once
- **Best Use Cases**:
  - Service implementation and refactoring
  - BullMQ queue pattern implementation
  - TypeScript strict mode compliance
  - Docker Compose orchestration
- **Optimal Strategy**: Use with copilot-instructions.md (975 tokens) + targeted file selection
- **Effectiveness Score**: 95/100 â­â­â­â­â­

---

#### 2ï¸âƒ£ **OpenAI GPT-4 Turbo (2024-Q4)**
- **Context Window**: 128K tokens input
- **Codebase Coverage**: 0.90% per request
- **Output**: 16K tokens (higher output than Claude)
- **Strengths**:
  - âœ… Excellent code generation quality
  - âœ… Strong with Node.js/Express patterns
  - âœ… Good at following architectural constraints
  - âœ… Better output token limit for large file generation
- **Weaknesses**:
  - âš ï¸ Smaller context than Claude 3.5
  - âš ï¸ Sometimes verbose in explanations
  - âš ï¸ Requires 111 context windows for full codebase
- **Best Use Cases**:
  - REST API endpoint generation
  - Large file scaffolding
  - OpenAPI specification generation
  - Python service implementation
- **Optimal Strategy**: Chunk work by service boundaries, use copilot-instructions.md
- **Effectiveness Score**: 92/100 â­â­â­â­â­

---

#### 3ï¸âƒ£ **Google Gemini 1.5 Pro**
- **Context Window**: 2M tokens input ğŸš€
- **Codebase Coverage**: 14.08% per request (BEST)
- **Output**: 8K tokens
- **Strengths**:
  - âœ… MASSIVE context window - can see large portions of codebase
  - âœ… Excellent for codebase-wide refactoring
  - âœ… Can analyze entire service mesh patterns
  - âœ… Best for understanding cross-service dependencies
  - âœ… Strong with GCP/Cloud Run deployment
- **Weaknesses**:
  - âš ï¸ Still requires 8 windows for full codebase
  - âš ï¸ Sometimes less precise than Claude for code generation
  - âš ï¸ Slower response times with large contexts
- **Best Use Cases**:
  - Architecture-wide refactoring
  - Cross-service dependency analysis
  - Migration planning (entire service mesh)
  - Documentation generation across services
- **Optimal Strategy**: Load entire service directories + docs for holistic analysis
- **Effectiveness Score**: 94/100 â­â­â­â­â­

---

### **TIER A - HIGHLY EFFECTIVE**

#### 4ï¸âƒ£ **DeepSeek Coder V2 (33B)**
- **Context Window**: 128K tokens input
- **Codebase Coverage**: 0.90% per request
- **Output**: 4K tokens
- **Strengths**:
  - âœ… Excellent code completion
  - âœ… Fast inference (critical for voice mode)
  - âœ… Strong with TypeScript/JavaScript
  - âœ… Cost-effective for high-volume usage
  - âœ… **ALREADY INTEGRATED** in reasoning-gateway service
- **Weaknesses**:
  - âš ï¸ Smaller output window
  - âš ï¸ Less sophisticated architectural reasoning
- **Best Use Cases**:
  - Real-time code completion in voice mode
  - Quick code fixes and refactoring
  - In-IDE assistance
  - High-frequency, low-latency requests
- **Optimal Strategy**: Use for tactical coding, delegate strategic to Claude/Gemini
- **Effectiveness Score**: 88/100 â­â­â­â­

---

#### 5ï¸âƒ£ **OpenAI GPT-4o (Omni)**
- **Context Window**: 128K tokens input
- **Codebase Coverage**: 0.90% per request
- **Output**: 16K tokens
- **Strengths**:
  - âœ… Multimodal (can analyze UI screenshots)
  - âœ… Fast response times
  - âœ… Good code quality
  - âœ… Excellent for frontend work
- **Weaknesses**:
  - âš ï¸ Similar limitations to GPT-4 Turbo
  - âš ï¸ Multimodal features not critical for backend services
- **Best Use Cases**:
  - Frontend component development
  - UI/UX analysis from screenshots
  - Full-stack feature implementation
  - Product page spec implementation
- **Optimal Strategy**: Use for vibe-cockpit frontend, delegate backend to specialized models
- **Effectiveness Score**: 87/100 â­â­â­â­

---

#### 6ï¸âƒ£ **Anthropic Claude 3 Opus**
- **Context Window**: 200K tokens input
- **Codebase Coverage**: 1.41% per request
- **Output**: 4K tokens
- **Strengths**:
  - âœ… Superior reasoning and planning
  - âœ… Excellent for architecture design
  - âœ… Best for complex problem decomposition
  - âœ… Strong with compliance requirements
- **Weaknesses**:
  - âš ï¸ Slower than Claude 3.5 Sonnet
  - âš ï¸ More expensive
  - âš ï¸ Lower output token limit
- **Best Use Cases**:
  - Architecture decision records (ADRs)
  - Complex compliance logic (cannabis regulations)
  - Strategic planning and system design
  - Security analysis
- **Optimal Strategy**: Use for planning phase, execute with Claude 3.5 Sonnet
- **Effectiveness Score**: 90/100 â­â­â­â­â­

---

### **TIER B - EFFECTIVE FOR SPECIFIC TASKS**

#### 7ï¸âƒ£ **Meta Llama 3.1 (405B)**
- **Context Window**: 128K tokens input
- **Codebase Coverage**: 0.90% per request
- **Output**: 4K tokens
- **Strengths**:
  - âœ… Open source (can self-host)
  - âœ… Good code generation
  - âœ… Strong with Python services
  - âœ… Cost-effective if self-hosted
- **Weaknesses**:
  - âš ï¸ Requires significant infrastructure to run
  - âš ï¸ Less polished than commercial models
- **Best Use Cases**:
  - Python backend services
  - Compliance service implementation
  - Self-hosted scenarios (data sovereignty)
- **Effectiveness Score**: 82/100 â­â­â­â­

---

#### 8ï¸âƒ£ **Mistral Large 2 (123B)**
- **Context Window**: 128K tokens input
- **Codebase Coverage**: 0.90% per request
- **Output**: 4K tokens
- **Strengths**:
  - âœ… Strong European option (GDPR compliance)
  - âœ… Good code quality
  - âœ… Cost-effective
- **Weaknesses**:
  - âš ï¸ Less documentation training than OpenAI/Anthropic
  - âš ï¸ Smaller ecosystem
- **Best Use Cases**:
  - European deployment scenarios
  - Cost-sensitive projects
- **Effectiveness Score**: 78/100 â­â­â­â­

---

#### 9ï¸âƒ£ **Cohere Command R+**
- **Context Window**: 128K tokens input
- **Codebase Coverage**: 0.90% per request
- **Output**: 4K tokens
- **Strengths**:
  - âœ… Strong RAG capabilities
  - âœ… Good for documentation search
  - âœ… Enterprise features
- **Weaknesses**:
  - âš ï¸ Less specialized for code
  - âš ï¸ Better for search than generation
- **Best Use Cases**:
  - Documentation retrieval
  - Semantic search across 4,032 markdown files
- **Effectiveness Score**: 75/100 â­â­â­

---

### **TIER C - LIMITED EFFECTIVENESS**

#### ğŸ”Ÿ **OpenAI GPT-3.5 Turbo**
- **Context Window**: 16K tokens input
- **Codebase Coverage**: 0.11% per request
- **Output**: 4K tokens
- **Strengths**:
  - âœ… Very fast
  - âœ… Very cheap
  - âœ… Good for simple tasks
- **Weaknesses**:
  - âš ï¸ Context too small for this codebase
  - âš ï¸ Cannot handle microservices complexity
- **Effectiveness Score**: 55/100 â­â­â­

---

## ğŸ¯ RECOMMENDED MODEL STRATEGY FOR LIVHANA-SOT

### **Multi-Model Ensemble Approach** (BEST PRACTICE)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LIVHANA-SOT AI MODEL STRATEGY                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ­ PLANNING & ARCHITECTURE
â”œâ”€ Claude 3 Opus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ADRs, system design, compliance
â”œâ”€ Gemini 1.5 Pro â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Cross-service analysis, migrations
â””â”€ copilot-instructions.md â”€â”€â†’ All models (975 token context boost)

âš¡ DEVELOPMENT & IMPLEMENTATION  
â”œâ”€ Claude 3.5 Sonnet â”€â”€â”€â”€â”€â”€â”€â”€â†’ PRIMARY (TypeScript, microservices)
â”œâ”€ GPT-4 Turbo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ SECONDARY (Python, large files)
â”œâ”€ DeepSeek Coder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Real-time (voice mode, in-IDE)
â””â”€ GPT-4o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Frontend (vibe-cockpit, UI work)

ğŸ” ANALYSIS & SEARCH
â”œâ”€ Gemini 1.5 Pro â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Codebase-wide analysis (2M context)
â”œâ”€ Cohere Command R+ â”€â”€â”€â”€â”€â”€â”€â”€â†’ Documentation search (4,032 .md files)
â””â”€ Claude 3.5 Sonnet â”€â”€â”€â”€â”€â”€â”€â”€â†’ Service-specific deep dives

ï¿½ï¿½ TESTING & VALIDATION
â”œâ”€ Claude 3.5 Sonnet â”€â”€â”€â”€â”€â”€â”€â”€â†’ Playwright test generation
â”œâ”€ GPT-4 Turbo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Test coverage analysis
â””â”€ DeepSeek Coder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Quick test fixes

ğŸ“ DOCUMENTATION
â”œâ”€ Claude 3.5 Sonnet â”€â”€â”€â”€â”€â”€â”€â”€â†’ Technical docs, READMEs
â”œâ”€ Gemini 1.5 Pro â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Architecture diagrams, ADRs
â””â”€ GPT-4 Turbo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ API documentation (OpenAPI specs)
```

---

## ğŸ’¡ BEST PRACTICES FOR THIS CODEBASE

### 1. **Always Use copilot-instructions.md**
```bash
# Every AI request should include:
System Context: .github/copilot-instructions.md (975 tokens)
+ Specific Files: <relevant service files>
= Maximum effectiveness with minimal token usage
```

**Impact**: 
- Saves ~50K tokens per session (context that would be rediscovered)
- Ensures consistency across all AI interactions
- Reduces hallucination by providing ground truth architecture

---

### 2. **Service-Boundary Chunking**
```
âŒ BAD: "Refactor the entire backend"
   (Requires 14.21M tokens - impossible in one request)

âœ… GOOD: "Refactor voice-service Redis queue integration"
   (Requires ~15K tokens - fits in any model)
```

**Strategy**:
- Chunk work by microservice boundaries (backend/voice-service/, etc.)
- Each service is self-contained with Dockerfile, package.json, README
- Use copilot-instructions.md for cross-service patterns

---

### 3. **Context Window Optimization**

#### For Claude 3.5 Sonnet (200K tokens):
```javascript
Priority 1: copilot-instructions.md (975 tokens)
Priority 2: Target service README.md (~2-5K tokens)
Priority 3: Service implementation files (~10-20K tokens)
Priority 4: Common utilities (backend/common/) (~5K tokens)
Priority 5: Related ADRs (~5-10K tokens)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~30-45K tokens (leaves 155K for conversation)
```

#### For Gemini 1.5 Pro (2M tokens):
```javascript
Priority 1: copilot-instructions.md (975 tokens)
Priority 2: Entire backend/ directory (~500K tokens)
Priority 3: All ADRs and architecture docs (~100K tokens)
Priority 4: Frontend code (~200K tokens)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~800K tokens (leaves 1.2M for conversation)
```

---

### 4. **Model Selection Decision Tree**

```
START: What type of work?
â”‚
â”œâ”€ ğŸ—ï¸ Architecture/Planning
â”‚  â”œâ”€ Multi-service changes? â”€â”€â†’ Gemini 1.5 Pro
â”‚  â”œâ”€ Complex reasoning? â”€â”€â”€â”€â”€â”€â†’ Claude 3 Opus
â”‚  â””â”€ New ADR? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Claude 3 Opus
â”‚
â”œâ”€ ğŸ’» Implementation
â”‚  â”œâ”€ TypeScript/Node.js? â”€â”€â”€â”€â”€â†’ Claude 3.5 Sonnet
â”‚  â”œâ”€ Python services? â”€â”€â”€â”€â”€â”€â”€â”€â†’ GPT-4 Turbo
â”‚  â”œâ”€ React frontend? â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ GPT-4o
â”‚  â””â”€ Quick fix? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ DeepSeek Coder
â”‚
â”œâ”€ ğŸ” Analysis
â”‚  â”œâ”€ Full codebase scan? â”€â”€â”€â”€â”€â†’ Gemini 1.5 Pro
â”‚  â”œâ”€ Doc search? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Cohere Command R+
â”‚  â””â”€ Service-specific? â”€â”€â”€â”€â”€â”€â”€â†’ Claude 3.5 Sonnet
â”‚
â””â”€ ğŸ“ Documentation
   â”œâ”€ Architecture docs? â”€â”€â”€â”€â”€â”€â†’ Gemini 1.5 Pro
   â”œâ”€ API specs? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ GPT-4 Turbo
   â””â”€ Service README? â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Claude 3.5 Sonnet
```

---

### 5. **Token Budget Management**

**Per-Session Budgets** (recommended):
```
ğŸ¯ Feature Implementation Session:
â”œâ”€ Context Loading: 30-50K tokens
â”œâ”€ Conversation: 50-100K tokens
â”œâ”€ Code Generation: 20-40K tokens
â””â”€ Testing/Validation: 10-20K tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 110-210K tokens per session

ğŸ’° Cost Optimization:
â”œâ”€ Use DeepSeek for high-frequency tasks ($0.14/1M tokens)
â”œâ”€ Use Claude 3.5 for critical work ($3/1M input, $15/1M output)
â”œâ”€ Use Gemini for bulk analysis ($1.25/1M input, $5/1M output)
â””â”€ Reserve Claude 3 Opus for planning only ($15/1M input, $75/1M output)
```

---

### 6. **Inter-Agent Communication**

**File-Based Task Queue Pattern** (already implemented):
```bash
tmp/agent_status/codex_tasks/
â”œâ”€â”€ task_<uuid>.request.json   # Liv Hana â†’ CODEX
â””â”€â”€ task_<uuid>.result.json    # CODEX â†’ Liv Hana
```

**Best Practice**:
- Use Claude 3.5 Sonnet for Liv Hana (voice orchestration)
- Use any model for CODEX (Cursor IDE execution)
- Coordinate via JSON task files (see .claude/INTER_AGENT_COMMUNICATION_PROTOCOL.md)

---

### 7. **Voice Mode Optimization**

**Current Setup**:
```
voice-service (Port 8080) â†’ BullMQ Queue â†’ reasoning-gateway (Port 4002)
                                                      â†“
                                              [DeepSeek Coder V2]
```

**Optimization**:
```javascript
// reasoning-gateway should support multiple models:
const models = {
  'deepseek': { latency: 'low', cost: 'low', quality: 'good' },
  'claude-3.5': { latency: 'medium', cost: 'medium', quality: 'excellent' },
  'gpt-4-turbo': { latency: 'medium', cost: 'medium', quality: 'excellent' }
};

// Route by query complexity:
if (prompt.length < 500 && !requiresMultiFile) {
  return deepseekCoder(prompt); // Fast, cheap
} else {
  return claude35Sonnet(prompt); // High quality
}
```

---

## ğŸ“ˆ EFFECTIVENESS METRICS

### Token Efficiency by Model

| Model | Context Window | Files Per Request | Services Per Request | Full Codebase Passes |
|-------|---------------|-------------------|---------------------|---------------------|
| Gemini 1.5 Pro | 2M | ~800 | ~8 | 8 passes |
| Claude 3.5 Sonnet | 200K | ~80 | ~2 | 71 passes |
| GPT-4 Turbo | 128K | ~50 | ~1 | 111 passes |
| DeepSeek Coder | 128K | ~50 | ~1 | 111 passes |
| GPT-3.5 Turbo | 16K | ~6 | ~0.2 | 889 passes |

### Cost Efficiency (1M Input Tokens)

| Model | Input Cost | Output Cost | Best For |
|-------|-----------|-------------|----------|
| DeepSeek Coder | $0.14 | $0.28 | High-frequency, low-latency |
| Gemini 1.5 Pro | $1.25 | $5.00 | Bulk analysis, migrations |
| Claude 3.5 Sonnet | $3.00 | $15.00 | Critical development work |
| GPT-4 Turbo | $10.00 | $30.00 | Large file generation |
| Claude 3 Opus | $15.00 | $75.00 | Architecture planning only |

### Quality vs Speed Tradeoff

```
Quality (Code Correctness)
    â†‘
    â”‚     Claude 3 Opus â—
    â”‚     Claude 3.5 Sonnet â—
    â”‚     GPT-4 Turbo â—
    â”‚     Gemini 1.5 Pro â—
    â”‚     GPT-4o â—
    â”‚     DeepSeek Coder â—
    â”‚     Llama 3.1 â—
    â”‚     Mistral Large 2 â—
    â”‚     Cohere Command R+ â—
    â”‚     GPT-3.5 Turbo â—
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Speed (Response Time)
```

---

## ï¿½ï¿½ IMMEDIATE ACTION ITEMS

### 1. âœ… **COMPLETED**: copilot-instructions.md
- File created: `.github/copilot-instructions.md`
- Size: 6.3 KB (975 tokens)
- Status: Ready for production use

### 2. ğŸ”„ **Update reasoning-gateway for Multi-Model Support**
```typescript
// backend/reasoning-gateway/src/models/index.ts
export const MODEL_REGISTRY = {
  'claude-3.5-sonnet': {
    provider: 'anthropic',
    contextWindow: 200_000,
    outputLimit: 8_000,
    costPer1M: { input: 3, output: 15 }
  },
  'gpt-4-turbo': {
    provider: 'openai', 
    contextWindow: 128_000,
    outputLimit: 16_000,
    costPer1M: { input: 10, output: 30 }
  },
  'gemini-1.5-pro': {
    provider: 'google',
    contextWindow: 2_000_000,
    outputLimit: 8_000,
    costPer1M: { input: 1.25, output: 5 }
  },
  'deepseek-coder-v2': {
    provider: 'deepseek',
    contextWindow: 128_000,
    outputLimit: 4_000,
    costPer1M: { input: 0.14, output: 0.28 }
  }
};
```

### 3. ğŸ“Š **Implement Token Usage Tracking**
```typescript
// backend/common/monitoring/token-tracker.ts
export class TokenTracker {
  async trackUsage(modelId: string, inputTokens: number, outputTokens: number) {
    const cost = calculateCost(modelId, inputTokens, outputTokens);
    await redis.hincrby('token:usage:daily', modelId, inputTokens + outputTokens);
    await redis.hincrby('token:cost:daily', modelId, Math.round(cost * 100));
  }
}
```

### 4. ğŸ”§ **Configure Model Selection API**
```typescript
// backend/reasoning-gateway/src/routes/infer.ts
POST /api/reasoning/infer
{
  "prompt": "string",
  "model": "auto" | "claude-3.5-sonnet" | "gpt-4-turbo" | "gemini-1.5-pro" | "deepseek-coder-v2",
  "priority": "speed" | "quality" | "cost",
  "context": ["file1.ts", "file2.ts"] // Optional: specific files to include
}
```

---

## ğŸ“š REFERENCES

### Internal Documentation
- **Architecture**: `docs/adr/1.6.2.1_3-6-1-5_ADR_002_Voice_Queue_Architecture_20251006.md`
- **CI Pipeline**: `docs/adr/1.6.2.1_3-6-1-5_ADR_003_Playwright_CI_Pipeline_20251006.md`
- **Inter-Agent Protocol**: `.claude/INTER_AGENT_COMMUNICATION_PROTOCOL.md`
- **Copilot Instructions**: `.github/copilot-instructions.md`

### Model Documentation (October 2025)
- Anthropic Claude 3.5: https://docs.anthropic.com/claude/docs
- OpenAI GPT-4 Turbo: https://platform.openai.com/docs/models
- Google Gemini 1.5: https://ai.google.dev/gemini-api/docs
- DeepSeek Coder: https://api-docs.deepseek.com/

---

**Generated**: October 27, 2025  
**Codebase**: RND-Technology/LivHana-SoT  
**Branch**: fix/mobile-control-po1  
**Maintained By**: AI Agent Collective + Jesse Niesen (CEO)

