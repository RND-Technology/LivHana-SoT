<!-- Optimized: 2025-10-06 -->
<!-- RPM: 1.6.2.1.1.6.2.1_SENTRY_ALERTS_20251006 -->
<!-- Session: E2E RPM DNA Application -->
<!-- AOM: RND (Reggie & Dro) -->
<!-- COI: TECHNOLOGY -->
<!-- RPM: HIGH -->
<!-- ACTION: BUILD -->

<!--
Optimized: 2025-10-03
RPM: 3.6.0.6.ops-technology-ship-status-documentation
Session: Dual-AI Collaboration - Sonnet Docs Sweep
-->
# Sentry Alert Configuration

## Overview

This document describes the recommended Sentry alert rules for LivHana backend services.

## Alert Rules

### 1. New Issue Alert

**Trigger**: A new error type is detected
**Severity**: P2 (Medium)
**Notification**: Slack #alerts channel

**Configuration**:

```yaml
When: A new issue is created
If: All environments
Then: Send notification to #alerts
```

**Purpose**: Catch new bugs immediately after deployment

---

### 2. High Volume Alert

**Trigger**: Error frequency exceeds threshold
**Severity**: P1 (High)
**Notification**: Slack #alerts + Email on-call

**Configuration**:

```yaml
When: An issue's frequency is more than 100 events in 1 hour
If: Environment is production
Then:
  - Send notification to #alerts
  - Send email to on-call engineer
```

**Purpose**: Detect widespread issues affecting many users

---

### 3. Regression Alert

**Trigger**: Previously resolved issue reappears
**Severity**: P2 (Medium)
**Notification**: Slack #engineering

**Configuration**:

```yaml
When: An issue changes state from resolved to unresolved
If: Environment is production
Then: Send notification to #engineering
```

**Purpose**: Catch regressions and incomplete fixes

---

### 4. User Impact Alert

**Trigger**: Error affects many unique users
**Severity**: P1 (High)
**Notification**: Slack #alerts + PagerDuty

**Configuration**:

```yaml
When: An issue is seen by more than 10 users in 1 hour
If: Environment is production
Then:
  - Send notification to #alerts
  - Trigger PagerDuty incident
```

**Purpose**: Prioritize errors with high user impact

---

### 5. Critical Path Alert

**Trigger**: Error occurs in critical business flow
**Severity**: P0 (Critical)
**Notification**: PagerDuty + Slack #incidents

**Configuration**:

```yaml
When: An issue's tags match critical-path
If: Environment is production AND tags contain "critical-path"
Then:
  - Trigger PagerDuty incident
  - Send notification to #incidents
```

**Tags to use**:

- `critical-path:checkout`
- `critical-path:payment`
- `critical-path:authentication`

**Purpose**: Immediate response for business-critical errors

---

### 6. Performance Degradation Alert

**Trigger**: Transaction duration exceeds threshold
**Severity**: P2 (Medium)
**Notification**: Slack #engineering

**Configuration**:

```yaml
When: The 95th percentile transaction duration is greater than 2000ms
If: Environment is production
Then: Send notification to #engineering
```

**Purpose**: Detect performance regressions

---

### 7. N+1 Query Alert

**Trigger**: N+1 query pattern detected
**Severity**: P2 (Medium)
**Notification**: Slack #engineering

**Configuration**:

```yaml
When: A performance issue of type "N+1 Query" is detected
If: All environments
Then: Send notification to #engineering
```

**Purpose**: Catch inefficient database queries

---

### 8. Error Rate Spike Alert

**Trigger**: Sudden increase in error rate
**Severity**: P1 (High)
**Notification**: Slack #alerts + Email

**Configuration**:

```yaml
When: An issue's frequency increases by 50% in 1 hour
If: Environment is production
Then:
  - Send notification to #alerts
  - Send email to on-call engineer
```

**Purpose**: Detect sudden increases in errors

---

## Notification Channels

### Slack Integration

1. Go to Settings → Integrations → Slack
2. Authorize Sentry for your workspace
3. Map channels:
   - `#alerts` - High priority alerts
   - `#engineering` - Medium/low priority alerts
   - `#incidents` - Critical incidents only

### Email

1. Go to Settings → Integrations → Email
2. Configure recipients:
   - On-call engineer (from PagerDuty schedule)
   - Engineering manager
   - DevOps team

### PagerDuty Integration

1. Go to Settings → Integrations → PagerDuty
2. Enter Integration Key from PagerDuty
3. Configure for P0 and P1 alerts only

---

## Alert Tuning

### Issue Grouping

Configure fingerprinting rules to group related errors:

```javascript
// .sentryclirc
[fingerprinting]
rules = [
  {
    "matchers": [["type", "DatabaseError"]],
    "fingerprint": ["{{ default }}", "{{ transaction }}"]
  },
  {
    "matchers": [["message", "ECONNREFUSED"]],
    "fingerprint": ["connection-error", "{{ transaction }}"]
  }
]
```

### Issue Filtering

Filter out noise:

```javascript
// In sentry.js beforeSend
beforeSend(event, hint) {
  // Ignore known bot errors
  if (event.request?.headers?.['user-agent']?.includes('bot')) {
    return null;
  }

  // Ignore connection errors from known flaky networks
  const error = hint.originalException;
  if (error?.code === 'ECONNREFUSED' && error?.address === '192.168.1.1') {
    return null;
  }

  return event;
}
```

### Rate Limiting

Configure per-issue rate limits:

1. Go to Project Settings → Issue Grouping
2. Set rate limit to 10 alerts per hour for P2/P3 issues
3. No rate limit for P0/P1 issues

---

## Testing Alerts

### Test New Issue Alert

```bash
curl -X POST http://localhost:3005/test-error
```

Should appear in Sentry within seconds.

### Test High Volume Alert

```bash
# Generate 150 errors in quick succession
for i in {1..150}; do
  curl -X POST http://localhost:3005/test-error &
done
wait
```

Should trigger alert after ~100 errors.

### Test Performance Alert

```bash
# Simulate slow endpoint
curl -X GET "http://localhost:3005/api/slow-endpoint?delay=3000"
```

Should appear in performance monitoring.

---

## Alert Response Workflow

### When You Receive an Alert

1. **Acknowledge**: Click "Acknowledge" in Sentry or PagerDuty
2. **Assess**: Check severity, user impact, frequency
3. **Investigate**: Review stack trace, breadcrumbs, and context
4. **Communicate**: Post update in #incidents if P0/P1
5. **Resolve**: Apply fix or mitigation
6. **Monitor**: Watch for recurrence
7. **Document**: Add to post-mortem if significant

### Escalation

If unable to resolve within:

- **P0**: 15 minutes → Escalate to Senior Engineer
- **P1**: 1 hour → Escalate to Engineering Manager
- **P2**: 4 hours → Create ticket, schedule fix

---

## Monthly Review

Review alert effectiveness monthly:

1. **Alert Volume**: Are we getting too many/too few alerts?
2. **False Positives**: Any alerts that weren't actionable?
3. **Missed Issues**: Any issues that should have alerted but didn't?
4. **Response Times**: Are we meeting SLAs?
5. **Tuning**: Adjust thresholds based on learnings

---

## Best Practices

1. **Use Tags**: Tag errors with business context

   ```javascript
   Sentry.setTag('payment_processor', 'stripe');
   Sentry.setTag('user_tier', 'premium');
   ```

2. **Add Context**: Include relevant data

   ```javascript
   Sentry.setContext('order', {
     id: orderId,
     total: orderTotal,
     items: itemCount,
   });
   ```

3. **Set User**: Identify affected users

   ```javascript
   Sentry.setUser({
     id: user.id,
     email: user.email,
   });
   ```

4. **Breadcrumbs**: Add debugging trail

   ```javascript
   Sentry.addBreadcrumb({
     category: 'payment',
     message: 'Attempting payment charge',
     level: 'info',
   });
   ```

5. **Release Tracking**: Deploy with releases

   ```bash
   export SENTRY_RELEASE=$(git rev-parse HEAD)
   # Sentry will automatically track by release
   ```

---

## Cost Optimization

To stay within budget:

1. **Sample Performance**: Set `tracesSampleRate: 0.1` (10%)
2. **Filter Noise**: Use `beforeSend` to filter out non-actionable errors
3. **Rate Limit**: Set per-issue rate limits
4. **Archive Old Issues**: Regularly resolve/archive old issues
5. **Monitor Quota**: Set up quota alerts in Sentry

---

## Related Documentation

- [Monitoring Runbook](./MONITORING_RUNBOOK.md)
- [Monitoring Setup Guide](./MONITORING_SETUP.md)
- [Incident Response Procedures](./INCIDENT_RESPONSE.md)

---

**Last Updated**: October 2025
**Owner**: DevOps Team

<!-- Last verified: 2025-10-02 -->

<!-- Optimized: 2025-10-02 -->

<!-- Last updated: 2025-10-02 -->

<!-- Last optimized: 2025-10-02 -->
