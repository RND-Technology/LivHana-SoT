<!-- Optimized: 2025-10-06 -->
<!-- RPM: 1.6.2.1.1.6.2.1_GMAIL_INGEST_README_20251006 -->
<!-- Session: E2E RPM DNA Application -->
<!-- AOM: RND (Reggie & Dro) -->
<!-- COI: TECHNOLOGY -->
<!-- RPM: HIGH -->
<!-- ACTION: BUILD -->

# Gmail Ingestion Pipeline

Complete Gmail ingestion system with OAuth 2.0 authentication, BigQuery storage, and business intelligence extraction for LivHana cannabis operations.

## Overview

This pipeline ingests emails from multiple Gmail accounts into BigQuery for analysis, compliance tracking, and business intelligence.

**Accounts:**

- `jesseniesen@gmail.com` - Primary CEO account
- `high@reggieanddro.com` - R&D TX operations

**Features:**

- OAuth 2.0 authentication with automatic token refresh
- Multi-account support with 1Password integration
- Pagination for 1000+ emails per account
- Full content extraction (HTML → plain text, attachments)
- Thread relationship tracking
- BigQuery storage with full-text search
- Smart filtering and category detection
- Deduplication and incremental sync
- Rate limiting and exponential backoff
- Business intelligence extraction
- PII detection and masking
- Cloud Storage for attachments

## Quick Start

### 1. Prerequisites

```bash
# Install dependencies
cd automation/data-pipelines
npm install googleapis @google-cloud/bigquery @google-cloud/storage html-to-text p-limit open

# Set up environment variables
export GCP_PROJECT_ID="your-project-id"
export BQ_DATASET="communications"
export GCS_GMAIL_BUCKET="livhana-gmail-attachments"
```

### 2. Google Cloud Setup

#### Create OAuth 2.0 Credentials

1. Go to [Google Cloud Console](https://console.cloud.google.com/apis/credentials)
2. Create a new project or select existing
3. Enable Gmail API:
   - Go to "APIs & Services" > "Library"
   - Search for "Gmail API"
   - Click "Enable"
4. Create OAuth 2.0 credentials:
   - Go to "APIs & Services" > "Credentials"
   - Click "Create Credentials" > "OAuth client ID"
   - Application type: **Desktop app**
   - Name: "Gmail Ingestion Pipeline"
   - Click "Create"
5. Download JSON credentials
6. Save as:
   - `gmail_credentials_jessen.json` for <jesseniesen@gmail.com>
   - `gmail_credentials_high.json` for <high@reggieanddro.com>

#### Create BigQuery Dataset

```bash
# Create dataset
bq mk --dataset \
  --location=US \
  --description="Gmail communications data" \
  ${GCP_PROJECT_ID}:communications

# Create tables
bq query --use_legacy_sql=false < gmail_bigquery_schema.sql
```

#### Create Cloud Storage Bucket

```bash
# Create bucket for attachments
gsutil mb -l us -c STANDARD gs://livhana-gmail-attachments

# Set lifecycle policy (optional - delete files older than 7 years)
cat > lifecycle.json << EOF
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "Delete"},
        "condition": {"age": 2555}
      }
    ]
  }
}
EOF

gsutil lifecycle set lifecycle.json gs://livhana-gmail-attachments
```

### 3. Authentication

Authenticate each Gmail account:

```bash
# Authenticate Jessen's account
node gmail_auth.js --account=jesseniesen

# Authenticate R&D account
node gmail_auth.js --account=high
```

This will:

1. Open your browser for Google OAuth consent
2. Save the access token locally
3. Store refresh token for automatic renewal

**Security Note:** Token files contain sensitive credentials. Keep them secure and add to `.gitignore`.

### 4. Run Tests

```bash
# Verify setup
node gmail_test.js
```

Tests verify:

- Credentials and tokens exist
- OAuth client creation
- Gmail API connection
- Message fetching and parsing
- BigQuery schema
- Category detection

### 5. Run Ingestion

```bash
# Initial full sync (all accounts)
node gmail_ingest.js --full

# Incremental sync (only new emails)
node gmail_ingest.js

# Single account sync
node gmail_ingest.js --account=jesseniesen@gmail.com

# Limit number of messages
node gmail_ingest.js --max=500
```

## Usage

### Command Line Options

```bash
node gmail_ingest.js [OPTIONS]

Options:
  --full                 Full sync (re-ingest all emails)
  --account=EMAIL        Sync specific account only
  --max=N                Maximum messages to fetch (default: 1000)

Examples:
  node gmail_ingest.js --full --max=5000
  node gmail_ingest.js --account=jesseniesen@gmail.com
  node gmail_ingest.js
```

### Programmatic Usage

```javascript
import { ingestAccount, ACCOUNTS } from './gmail_ingest.js';

// Ingest specific account
const account = ACCOUNTS[0]; // jesseniesen@gmail.com
const result = await ingestAccount(account, {
  fullSync: false,
  maxMessages: 1000
});

console.log(`Processed: ${result.processed}`);
console.log(`Skipped: ${result.skipped}`);
```

## Architecture

### Data Flow

```
Gmail API
    ↓
OAuth 2.0 Authentication
    ↓
Message Fetching (paginated)
    ↓
Content Extraction
    ├─→ HTML → Plain Text
    ├─→ Parse Headers
    ├─→ Extract Attachments
    └─→ Detect Categories
    ↓
Deduplication Check
    ↓
PII Masking
    ↓
Upload Attachments → Cloud Storage
    ↓
Insert into BigQuery
    ├─→ gmail_messages
    ├─→ gmail_attachments
    └─→ gmail_threads
    ↓
Build Thread Summaries
    ↓
Update Sync State
```

### BigQuery Schema

#### Table: `gmail_messages`

Primary table storing all email messages with full metadata and content.

**Key Fields:**

- `message_hash` - SHA-256 hash for deduplication
- `message_id` - Gmail message ID
- `thread_id` - Gmail thread ID
- `account_email` - Account email address
- `subject`, `from_email`, `to_email` - Email metadata
- `body_text`, `body_html` - Email content
- `categories[]` - Detected categories
- `sender_score` - Importance score (0-20)
- `labels[]` - Gmail labels (INBOX, SENT, etc)

**Partitioning:** By `timestamp` (daily)
**Clustering:** By `account_email`, `thread_id`

#### Table: `gmail_threads`

Thread-level summaries with aggregated metadata.

**Key Fields:**

- `thread_id` - Gmail thread ID
- `message_count` - Number of messages in thread
- `participant_emails[]` - All participants
- `first_message_date`, `last_message_date` - Thread timeline
- `categories[]` - Aggregated categories

#### Table: `gmail_attachments`

Attachment metadata with Cloud Storage references.

**Key Fields:**

- `message_id` - Parent message ID
- `filename` - Attachment filename
- `mime_type` - File type
- `size_bytes` - File size
- `gcs_path` - Cloud Storage path (gs://...)

### Category Detection

Emails are automatically categorized using keyword detection:

**Categories:**

- `cannabis_business` - Cannabis operations, dispensary, cultivation
- `compliance` - Regulatory, licensing, inspections
- `legal` - Legal matters, attorneys, contracts
- `financial` - Invoices, payments, banking
- `important` - Starred or marked important
- `inbox` - In inbox
- `sent` - Sent emails
- `general` - Default fallback

### Sender Scoring

Emails receive importance scores (0-20) based on:

- **Domain scoring (+10):** Government (.gov), state agencies, company domains
- **Label scoring (+5 each):** Important, starred labels
- **Interaction history (+0.1 per email, max +5):** Frequent senders

### PII Masking

Automatically masks sensitive data in email bodies:

- **SSN:** `XXX-XX-XXXX`
- **Credit cards:** `XXXX-XXXX-XXXX-XXXX`
- **Phone numbers:** `XXX-XXX-XXXX`

## BigQuery Queries

### Search All Emails

```sql
SELECT
  message_id,
  account_email,
  subject,
  from_email,
  timestamp,
  snippet
FROM `communications.gmail_messages`
WHERE
  SEARCH(body_text, 'cannabis license')
  AND is_spam = FALSE
ORDER BY timestamp DESC
LIMIT 100;
```

### Recent Important Emails

```sql
SELECT * FROM `communications.gmail_recent_important`
WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
ORDER BY sender_score DESC, timestamp DESC;
```

### Cannabis Business Communications

```sql
SELECT * FROM `communications.gmail_cannabis_business`
WHERE timestamp >= '2025-01-01'
ORDER BY timestamp DESC;
```

### Compliance & Legal Emails

```sql
SELECT * FROM `communications.gmail_compliance_legal`
WHERE has_attachments = TRUE
ORDER BY timestamp DESC;
```

### Email Volume by Day

```sql
SELECT * FROM `communications.gmail_daily_volume`
WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
ORDER BY date DESC;
```

### Top Senders

```sql
SELECT * FROM `communications.gmail_top_senders`
WHERE account_email = 'jesseniesen@gmail.com'
ORDER BY email_count DESC
LIMIT 20;
```

### Find Emails with Attachments

```sql
SELECT
  m.message_id,
  m.subject,
  m.from_email,
  m.timestamp,
  ARRAY_AGG(a.filename) as attachments
FROM `communications.gmail_messages` m
JOIN `communications.gmail_attachments` a
  ON m.message_hash = a.message_hash
WHERE m.account_email = 'jesseniesen@gmail.com'
GROUP BY m.message_id, m.subject, m.from_email, m.timestamp
ORDER BY m.timestamp DESC;
```

## Automation

### Daily Incremental Sync

Set up Cloud Scheduler to run daily:

```bash
# Create Cloud Scheduler job
gcloud scheduler jobs create http gmail-daily-sync \
  --schedule="0 2 * * *" \
  --uri="https://us-central1-${GCP_PROJECT_ID}.cloudfunctions.net/gmail-ingest" \
  --http-method=POST \
  --location=us-central1 \
  --time-zone="America/Chicago"
```

### Cloud Function (Optional)

Deploy as Cloud Function for serverless execution:

```javascript
// index.js
import { ingestAccount, ACCOUNTS } from './gmail_ingest.js';

export async function gmailIngest(req, res) {
  const results = [];

  for (const account of ACCOUNTS) {
    try {
      const result = await ingestAccount(account, {
        fullSync: false,
        maxMessages: 1000
      });
      results.push({ account: account.email, ...result });
    } catch (error) {
      results.push({ account: account.email, error: error.message });
    }
  }

  res.json({ success: true, results });
}
```

Deploy:

```bash
gcloud functions deploy gmail-ingest \
  --runtime=nodejs20 \
  --trigger-http \
  --allow-unauthenticated \
  --entry-point=gmailIngest \
  --memory=1024MB \
  --timeout=540s
```

## Security & Privacy

### Token Storage

Tokens are stored locally with restricted permissions (0600). For production:

1. **1Password Integration:**

   ```bash
   # Store credentials in 1Password
   op item create \
     --category=secure-note \
     --title="Gmail API Credentials - Jessen" \
     --vault="Engineering"

   # Reference in credentials file
   echo "op://Engineering/Gmail API Credentials - Jessen/credential" > gmail_credentials_jessen.json
   ```

2. **Secret Manager (recommended):**

   ```bash
   # Store token in Secret Manager
   gcloud secrets create gmail-token-jessen \
     --data-file=gmail_token_jessen.json

   # Access in code
   const token = await secretManager.accessSecretVersion({
     name: 'projects/PROJECT_ID/secrets/gmail-token-jessen/versions/latest'
   });
   ```

### PII Protection

- Email bodies are automatically scanned for PII
- SSN, credit cards, phone numbers are masked
- Original content stored separately for audit
- Access logs maintained in BigQuery

### Access Controls

```bash
# Grant read-only access to analysts
bq add-iam-policy-binding \
  --member='user:analyst@livhana.com' \
  --role='roles/bigquery.dataViewer' \
  ${GCP_PROJECT_ID}:communications

# Grant attachment access
gsutil iam ch \
  user:analyst@livhana.com:objectViewer \
  gs://livhana-gmail-attachments
```

## Troubleshooting

### Authentication Errors

**Error:** `Token has been expired or revoked`

**Solution:**

```bash
# Re-authenticate account
node gmail_auth.js --account=jesseniesen
```

### Rate Limit Errors

**Error:** `Rate limit exceeded`

**Solution:**

- Pipeline automatically retries with exponential backoff
- Reduce `CONCURRENT_OPERATIONS` in code
- Increase delay between requests

### Missing Credentials

**Error:** `Credentials file not found`

**Solution:**

1. Download OAuth credentials from Google Cloud Console
2. Save as `gmail_credentials_jessen.json` or `gmail_credentials_high.json`
3. Run authentication: `node gmail_auth.js --account=jesseniesen`

### BigQuery Errors

**Error:** `Table not found`

**Solution:**

```bash
# Create tables
bq query --use_legacy_sql=false < gmail_bigquery_schema.sql
```

**Error:** `Permission denied`

**Solution:**

```bash
# Grant BigQuery permissions
gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member="serviceAccount:YOUR_SERVICE_ACCOUNT" \
  --role="roles/bigquery.dataEditor"
```

## Monitoring

### Check Sync State

```bash
# View sync state files
cat .gmail_sync_jessen.json
cat .gmail_sync_high.json
```

### View Logs

```bash
# Run with verbose logging
node gmail_ingest.js 2>&1 | jq .

# Filter errors only
node gmail_ingest.js 2>&1 | jq 'select(.level == "ERROR")'
```

### BigQuery Monitoring

```sql
-- Messages ingested today
SELECT
  account_email,
  COUNT(*) as messages_ingested,
  MIN(timestamp) as earliest_message,
  MAX(timestamp) as latest_message
FROM `communications.gmail_messages`
WHERE DATE(ingested_at) = CURRENT_DATE()
GROUP BY account_email;

-- Attachment storage usage
SELECT
  account_email,
  COUNT(*) as attachment_count,
  SUM(size_bytes) / 1024 / 1024 / 1024 as total_gb
FROM `communications.gmail_attachments`
GROUP BY account_email;

-- Ingestion errors (check logs)
SELECT *
FROM `communications.gmail_messages`
WHERE ingested_at IS NULL
LIMIT 100;
```

## Performance

### Optimization Tips

1. **Incremental Sync:** Always use incremental sync for daily updates
2. **Concurrent Operations:** Adjust `CONCURRENT_OPERATIONS` based on rate limits
3. **Batch Size:** Default 500 rows per batch works well for most cases
4. **Partitioning:** Queries benefit from partitioning by date
5. **Clustering:** Cluster by `account_email` and `thread_id` for thread queries

### Benchmarks

- **Initial sync:** ~1000 emails/minute per account
- **Incremental sync:** ~2000 emails/minute per account
- **Attachment download:** ~10-20 MB/s
- **BigQuery insert:** ~500 rows/second

## Maintenance

### Weekly Tasks

```bash
# Verify data integrity
node gmail_test.js

# Check sync state
for f in .gmail_sync_*.json; do
  echo "$f:"
  jq . "$f"
done
```

### Monthly Tasks

```bash
# Analyze storage usage
gsutil du -sh gs://livhana-gmail-attachments

# Optimize BigQuery tables
bq query --use_legacy_sql=false "
  SELECT
    table_name,
    ROUND(size_bytes / 1024 / 1024 / 1024, 2) as size_gb,
    row_count
  FROM \`communications.__TABLES__\`
  WHERE table_name LIKE 'gmail_%'
"
```

### Backup

```bash
# Export BigQuery tables
bq extract \
  --destination_format=AVRO \
  communications.gmail_messages \
  gs://livhana-backups/gmail/messages_$(date +%Y%m%d).avro

# Backup tokens (encrypted)
tar czf gmail_tokens_backup.tar.gz .gmail_sync_*.json gmail_token_*.json
gpg --encrypt --recipient your-email@example.com gmail_tokens_backup.tar.gz
```

## Support

For issues or questions:

1. Check troubleshooting section above
2. Review logs with `jq` for structured output
3. Run test suite: `node gmail_test.js`
4. Verify credentials and tokens are valid
5. Check Gmail API quotas in Google Cloud Console

## Files

- `gmail_ingest.js` - Main ingestion pipeline
- `gmail_auth.js` - OAuth authentication
- `gmail_test.js` - Test suite
- `gmail_bigquery_schema.sql` - BigQuery schema
- `gmail_credentials_*.json` - OAuth credentials (not in git)
- `gmail_token_*.json` - Access tokens (not in git)
- `.gmail_sync_*.json` - Sync state (not in git)

## License

Proprietary - LivHana Inc.

<!-- Last verified: 2025-10-02 -->

<!-- Optimized: 2025-10-02 -->

<!-- Last updated: 2025-10-02 -->

<!-- Last optimized: 2025-10-02 -->
