<!-- Optimized: 2025-10-06 -->
<!-- RPM: 1.6.2.1.1.6.2.1_NOTION_QUICKSTART_20251006 -->
<!-- Session: E2E RPM DNA Application -->
<!-- AOM: RND (Reggie & Dro) -->
<!-- COI: TECHNOLOGY -->
<!-- RPM: HIGH -->
<!-- ACTION: BUILD -->

# Notion Ingestion - Quick Start Guide

Complete Notion workspace ingestion to BigQuery and markdown files in 5 minutes.

## Files Created

```
automation/data-pipelines/
├── notion_ingest.js              # Main ingestion script (production-ready)
├── notion_test.js                # Connectivity test script
├── notion_setup.sh               # Interactive setup script
├── .env.notion                   # Environment variables template
├── NOTION_INGEST_README.md       # Full documentation
├── NOTION_BIGQUERY_QUERIES.sql   # Useful BigQuery queries
└── NOTION_QUICKSTART.md          # This file
```

## Prerequisites (5 minutes)

### 1. Create Notion Integration

1. Go to <https://www.notion.so/my-integrations>
2. Click "+ New integration"
3. Name it (e.g., "LivHana Data Pipeline")
4. Select your workspace
5. Click "Submit"
6. Copy the "Internal Integration Token" (starts with `secret_`)

### 2. Share Pages with Integration

1. Open Notion
2. Go to a page you want to ingest
3. Click "..." menu → "Add connections"
4. Select your integration
5. Repeat for all pages (or share at workspace level)

### 3. Set Up Google Cloud

Already configured if you have:

- BigQuery API enabled
- Application default credentials set up
- Or service account key file

```bash
# Quick setup:
gcloud auth application-default login
```

## Quick Start (3 commands)

### Step 1: Install Dependencies

```bash
cd automation/data-pipelines
npm install
```

### Step 2: Set Environment Variables

```bash
export NOTION_API_KEY=secret_yourNotionIntegrationKey
export BQ_DATASET=knowledge  # Optional, defaults to "knowledge"
```

Or create `.env` file:

```bash
cp .env.notion .env
# Edit .env and add your NOTION_API_KEY
```

### Step 3: Test & Run

Test connectivity:

```bash
npm run notion:test
```

Run ingestion:

```bash
npm run notion:ingest
```

## What Happens

1. **Searches** all pages and databases in workspace
2. **Extracts** full content (all blocks, nested content)
3. **Converts** to markdown
4. **Exports** to `data/notion_export/*.md`
5. **Creates** BigQuery table `knowledge.notion_pages`
6. **Inserts** structured data with full content

## Output Locations

### Markdown Files

```
data/notion_export/
├── product_roadmap_abc123.md
├── meeting_notes_def456.md
└── technical_docs_ghi789.md
```

### BigQuery Table

```
knowledge.notion_pages
- 19 columns including page_id, title, content_markdown, properties
- Searchable full-text content
- All metadata and timestamps
```

## Verify Results

### Check Markdown Files

```bash
ls -lh data/notion_export/
cat data/notion_export/*.md | head -50
```

### Query BigQuery

```bash
bq query --use_legacy_sql=false \
  'SELECT title, content_length, last_edited_time
   FROM knowledge.notion_pages
   LIMIT 10'
```

Or in Console: <https://console.cloud.google.com/bigquery>

## Common Issues

### "NOTION_API_KEY not set"

```bash
export NOTION_API_KEY=secret_your_key_here
```

### "Page not found" errors

- Share more pages with your integration
- Check integration has access in Notion settings

### BigQuery permission errors

```bash
gcloud auth application-default login
# Or set service account:
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json
```

### Rate limit (429) errors

- Script automatically retries with exponential backoff
- For very large workspaces, this is normal

## Next Steps

### Schedule Daily Ingestion

Add to crontab:

```bash
0 2 * * * cd /path/to/automation/data-pipelines && node notion_ingest.js >> /var/log/notion.log 2>&1
```

### Search Content in BigQuery

```sql
-- Find pages about "product launch"
SELECT title, url, content_length
FROM knowledge.notion_pages
WHERE LOWER(content_markdown) LIKE '%product launch%'
ORDER BY last_edited_time DESC;
```

See `NOTION_BIGQUERY_QUERIES.sql` for 30+ useful queries.

### Query with bq CLI

```bash
bq query --use_legacy_sql=false \
  "SELECT title, url FROM knowledge.notion_pages
   WHERE LOWER(title) LIKE '%meeting%' LIMIT 10"
```

## Advanced Usage

### Custom Dataset

```bash
export BQ_DATASET=my_custom_dataset
npm run notion:ingest
```

### Filter Specific Pages

Edit `notion_ingest.js` line 387:

```javascript
const response = await notion.search({
  filter: { property: 'object', value: 'page' },
  query: 'meeting notes',  // Add search term
  start_cursor: startCursor,
  page_size: 100
});
```

### Add Retry Delay

Edit line 13:

```javascript
const RETRY_DELAY_MS = 2000;  // 2 seconds instead of 1
```

## Performance

| Workspace Size | Estimated Time |
|----------------|----------------|
| 50 pages       | 2-3 minutes    |
| 500 pages      | 20-30 minutes  |
| 5000 pages     | 3-4 hours      |

## Support

See full documentation: `NOTION_INGEST_README.md`

Sample queries: `NOTION_BIGQUERY_QUERIES.sql`

Test connectivity: `npm run notion:test`

## Architecture

```
┌─────────────┐
│   Notion    │
│  Workspace  │
└──────┬──────┘
       │
       │ API
       ▼
┌─────────────────┐
│ notion_ingest.js│
│                 │
│ - Search pages  │
│ - Fetch blocks  │
│ - Convert MD    │
└─────┬───────┬───┘
      │       │
      │       └───────────┐
      ▼                   ▼
┌──────────┐      ┌──────────────┐
│ Markdown │      │   BigQuery   │
│  Files   │      │ knowledge.   │
│          │      │ notion_pages │
└──────────┘      └──────────────┘
```

## Features Checklist

- [x] Notion API integration (@notionhq/client)
- [x] Search all pages and databases
- [x] Full block extraction (recursive)
- [x] 30+ block types supported
- [x] Markdown export
- [x] BigQuery integration
- [x] Schema auto-creation
- [x] Pagination handling (100 items/request)
- [x] Error handling & retry logic
- [x] Structured logging
- [x] Idempotent operation
- [x] Edge case handling
- [x] Production-ready code
- [x] Comprehensive documentation
- [x] Test suite
- [x] Setup script
- [x] Example queries

## Production Deployment

### Docker

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY automation/data-pipelines/package*.json ./
RUN npm ci --production
COPY automation/data-pipelines/*.js ./
CMD ["node", "notion_ingest.js"]
```

### Cloud Run / Cloud Functions

```bash
gcloud functions deploy notion-ingest \
  --runtime nodejs18 \
  --trigger-http \
  --entry-point main \
  --set-env-vars NOTION_API_KEY=$NOTION_API_KEY
```

### Kubernetes CronJob

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: notion-ingest
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: ingest
            image: gcr.io/project/notion-ingest:latest
            env:
            - name: NOTION_API_KEY
              valueFrom:
                secretKeyRef:
                  name: notion-credentials
                  key: api-key
          restartPolicy: OnFailure
```

---

**Ready to go?** Run `npm run notion:test` to verify setup, then `npm run notion:ingest` to start!

<!-- Last verified: 2025-10-02 -->

<!-- Optimized: 2025-10-02 -->

<!-- Last updated: 2025-10-02 -->

<!-- Last optimized: 2025-10-02 -->
