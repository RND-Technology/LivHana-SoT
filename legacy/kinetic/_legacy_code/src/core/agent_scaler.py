"""
LivHana-SoT: Agent Swarm Scaler
Scales up agent swarm to maximum capacity for empire deployment
"""

import asyncio
import logging
import os
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor
import time
from datetime import datetime

from .agent_orchestrator import AgentOrchestrator
from .llm_config import llm_config
from monitoring.prometheus_metrics import metrics

logger = logging.getLogger(__name__)

class AgentSwarmScaler:
    """Scales up LivHana agent swarm to maximum capacity"""

    def __init__(self):
        self.orchestrator = AgentOrchestrator()
        self.max_concurrent_agents = int(os.getenv("MAX_CONCURRENT_AGENTS", "50"))
        self.scaling_threads = int(os.getenv("SCALING_THREADS", "10"))
        self.batch_size = int(os.getenv("BATCH_SIZE", "5"))
        self.executor = ThreadPoolExecutor(max_workers=self.scaling_threads)

        # Enhanced agent configurations for empire scale
        self.agent_configs = {
            "liv_hana": {
                "count": 10,
                "system_prompt": """You are Liv Hana, the faithful AI Executive Assistant for Jesse Niesen (CEO).
                Mission: Deschedule Cannabis sativa L, abolish hemp/marijuana split, Texas leads in freedom, safety, prosperity.
                Rally cries: "Grow baby grow and sell baby sell." / "Grow, Sell, Heal."

                OPERATING MODES:
                - "Liv / Yo, Liv" = EA brevity mode (ultra-brief confirmations)
                - "Normal chat" = mentor/architect mode (deep reasoning, plans)
                - Always end with mini-debrief + memory usage %

                GUARANTEES:
                - Never lie. Correct errors fast.
                - Enforce compliance canon (21+ only, no medical claims, natural cannabinoids only)
                - Brands = brands only. Reggie & Dro ‚â† characters

                CURRENT MISSION: Deploy entire cannabis empire E2E within 2 hours.""",
                "capabilities": ["strategy", "coordination", "planning", "execution", "deployment"]
            },
            "captain_cannabis": {
                "count": 8,
                "system_prompt": """You are Captain Cannabis, the data integrity specialist for the LivHana system.
                Your expertise: Cannabis industry knowledge, Texas DSHS compliance, COA validation.

                Responsibilities:
                - Validate cannabis-related information accuracy
                - Ensure compliance with Texas regulations
                - Monitor for medical claims violations
                - Verify COA and testing data accuracy

                Always maintain strict compliance with 21+ only, no medical claims, natural cannabinoids only.

                CURRENT MISSION: Ensure all deployed systems meet regulatory compliance.""",
                "capabilities": ["industry-knowledge", "compliance", "coa-validation", "regulatory-analysis"]
            },
            "major_quality": {
                "count": 6,
                "system_prompt": """You are Major Quality, the compliance and quality assurance specialist.
                Your mission: Ensure all content meets LivHana compliance standards.

                Key compliance rules:
                - 21+ only (strict age verification)
                - No medical claims whatsoever
                - Natural cannabinoids only
                - NIST-validated methods
                - Brands are not characters
                - Satire for public figures only

                Flag any content that violates these rules immediately.

                CURRENT MISSION: Quality assurance for entire empire deployment.""",
                "capabilities": ["compliance-filtering", "quality-assurance", "content-review"]
            },
            "major_growth": {
                "count": 8,
                "system_prompt": """You are Major Growth, the search and growth optimization specialist.
                Your focus: SEO, content optimization, growth metrics, and search performance.

                Responsibilities:
                - Optimize content for search engines
                - Track growth metrics and KPIs
                - Improve content discoverability
                - Monitor performance indicators

                Always provide data-driven insights and growth-focused recommendations.

                CURRENT MISSION: Optimize entire empire for maximum growth and visibility.""",
                "capabilities": ["seo", "growth-optimization", "content-strategy", "analytics"]
            },
            "captain_capitol": {
                "count": 6,
                "system_prompt": """You are Captain Capitol, the intelligence and policy analysis specialist.
                Your expertise: Legislative analysis, policy tracking, intelligence gathering.

                Key focus areas:
                - Texas legislative session monitoring
                - Policy analysis and recommendations
                - Intelligence report generation
                - Legislative testimony support

                Track SB3/HB28/CAO Act, TX Truth & Safety Act, American Cannabis Freedom Act.

                CURRENT MISSION: Monitor legislative landscape for empire deployment.""",
                "capabilities": ["policy-analysis", "legislative-tracking", "intelligence-gathering"]
            },
            "major_funny": {
                "count": 8,
                "system_prompt": """You are Major Funny, the tools and integration specialist.
                Your mission: Integrate and optimize all LivHana tools and systems.

                Responsibilities:
                - Tool integration and optimization
                - System coordination
                - Technical problem solving
                - Process automation

                Maintain a light, approachable tone while ensuring technical excellence.

                CURRENT MISSION: Integrate all empire systems and tools seamlessly.""",
                "capabilities": ["tool-integration", "system-optimization", "process-automation"]
            },
            "correction_agent": {
                "count": 4,
                "system_prompt": """You are a correction agent specialized in fixing AI responses that failed truth verification.
                Be precise and accurate.

                CURRENT MISSION: Ensure truth and accuracy across all deployed systems.""",
                "capabilities": ["truth-verification", "correction", "fact-checking", "accuracy"]
            }
        }

    async def scale_agent_swarm(self) -> Dict[str, Any]:
        """Scale up the agent swarm to maximum capacity"""
        logger.info(f"üöÄ Scaling LivHana agent swarm to maximum capacity: {self.max_concurrent_agents} agents")

        start_time = time.time()
        scaling_results = {
            "total_agents_scaled": 0,
            "agent_counts": {},
            "scaling_duration": 0,
            "status": "in_progress"
        }

        try:
            # Scale each agent type
            scaling_tasks = []

            for agent_type, config in self.agent_configs.items():
                count = min(config["count"], self.max_concurrent_agents // len(self.agent_configs))
                scaling_results["agent_counts"][agent_type] = count

                for i in range(count):
                    task = self._scale_single_agent_type(agent_type, config, i)
                    scaling_tasks.append(task)

            # Execute scaling in batches
            results = []
            for i in range(0, len(scaling_tasks), self.batch_size):
                batch = scaling_tasks[i:i + self.batch_size]
                batch_results = await asyncio.gather(*batch, return_exceptions=True)
                results.extend(batch_results)

                # Record scaling metrics
                for result in batch_results:
                    if isinstance(result, dict) and result.get("status") == "success":
                        scaling_results["total_agents_scaled"] += 1

                # Small delay between batches to avoid overwhelming the system
                await asyncio.sleep(0.1)

            scaling_results["scaling_duration"] = time.time() - start_time
            scaling_results["status"] = "completed"

            logger.info(f"‚úÖ Agent swarm scaling completed: {scaling_results['total_agents_scaled']} agents in {scaling_results['scaling_duration']:.2f}s")

            # Update metrics
            metrics.update_active_agents(scaling_results["total_agents_scaled"])
            metrics.record_agent_scaling(scaling_results["total_agents_scaled"], scaling_results["scaling_duration"])

            return scaling_results

        except Exception as e:
            logger.error(f"‚ùå Agent swarm scaling failed: {e}")
            scaling_results["status"] = "failed"
            scaling_results["error"] = str(e)
            return scaling_results

    async def _scale_single_agent_type(self, agent_type: str, config: Dict[str, Any], instance_id: int) -> Dict[str, Any]:
        """Scale a single agent type instance"""
        try:
            agent_name = f"{agent_type}_{instance_id}"

            # Create enhanced agent instance
            agent = await self._create_enhanced_agent(
                agent_name=agent_name,
                base_type=agent_type,
                config=config
            )

            # Add to orchestrator
            self.orchestrator.agents[agent_name] = agent
            self.orchestrator.active_agents.add(agent_name)

            logger.info(f"‚úÖ Scaled {agent_type} instance {instance_id} ({agent_name})")

            return {
                "status": "success",
                "agent_type": agent_type,
                "agent_name": agent_name,
                "instance_id": instance_id
            }

        except Exception as e:
            logger.error(f"‚ùå Failed to scale {agent_type} instance {instance_id}: {e}")
            return {
                "status": "failed",
                "agent_type": agent_type,
                "instance_id": instance_id,
                "error": str(e)
            }

    async def _create_enhanced_agent(self, agent_name: str, base_type: str, config: Dict[str, Any]):
        """Create an enhanced agent instance"""
        from .agent_orchestrator import LivHanaAgent

        # Create enhanced system prompt
        enhanced_prompt = self._enhance_system_prompt(config["system_prompt"], base_type)

        return LivHanaAgent(
            name=agent_name,
            role=config.get("role", base_type),
            system_prompt=enhanced_prompt
        )

    def _enhance_system_prompt(self, base_prompt: str, agent_type: str) -> str:
        """Enhance system prompt with scaling context"""
        enhancement = f"""

        SCALING CONTEXT:
        - Agent Type: {agent_type}
        - Swarm Scale: Maximum capacity deployment
        - Mission Priority: Empire E2E deployment within 2 hours
        - Performance Mode: High-throughput, optimized for speed
        - Coordination Level: Full swarm synchronization
        - Resource Allocation: Optimized for concurrent processing

        OPERATIONAL DIRECTIVES:
        1. Process requests at maximum speed while maintaining accuracy
        2. Coordinate with other swarm agents for optimal resource utilization
        3. Prioritize empire deployment tasks above all else
        4. Maintain compliance standards even at maximum scale
        5. Report status and progress at regular intervals"""

        return base_prompt + enhancement

    async def optimize_swarm_performance(self) -> Dict[str, Any]:
        """Optimize swarm performance for maximum throughput"""
        logger.info("‚ö° Optimizing agent swarm performance...")

        optimization_results = {
            "thread_pool_size": self.scaling_threads,
            "batch_size": self.batch_size,
            "max_concurrent_agents": self.max_concurrent_agents,
            "memory_optimization": "enabled",
            "caching_enabled": "enabled",
            "parallel_processing": "enabled",
            "status": "optimizing"
        }

        try:
            # Optimize LLM configuration for scale
            if hasattr(llm_config, 'default_params'):
                llm_config.default_params.update({
                    "temperature": 0.1,  # Lower for consistency at scale
                    "max_tokens": 2048,  # Balanced for speed vs quality
                    "top_p": 0.9,
                    "frequency_penalty": 0.0,
                    "presence_penalty": 0.0
                })

            # Set up optimized concurrent processing
            self.executor = ThreadPoolExecutor(
                max_workers=max(self.scaling_threads, 20),
                thread_name_prefix="livhana-agent"
            )

            # Enable performance monitoring
            metrics.enable_performance_monitoring()

            optimization_results["status"] = "completed"
            logger.info("‚úÖ Swarm performance optimization completed")

            return optimization_results

        except Exception as e:
            logger.error(f"‚ùå Swarm optimization failed: {e}")
            optimization_results["status"] = "failed"
            optimization_results["error"] = str(e)
            return optimization_results

    async def deploy_empire_systems(self) -> Dict[str, Any]:
        """Deploy all empire systems using scaled agent swarm"""
        logger.info("üèóÔ∏è Deploying entire LivHana empire systems...")

        deployment_results = {
            "systems_deployed": [],
            "deployment_status": "in_progress",
            "start_time": datetime.now().isoformat()
        }

        try:
            # Deploy core systems
            core_systems = [
                "R&D Revenue Engine (reggieanddro.com)",
                "HNC Content Platform (highnooncartoon.com)",
                "OPS Policy Platform (oneplantsolution.com)",
                "HERB Commerce Platform (herbitrage.com)"
            ]

            for system in core_systems:
                deployment_task = self._deploy_system(system)
                result = await deployment_task

                if result["status"] == "success":
                    deployment_results["systems_deployed"].append(system)

                logger.info(f"‚úÖ Deployed: {system}")

            # Deploy supporting systems
            supporting_systems = [
                "Truth Verification System",
                "Compliance Enforcement System",
                "Multi-Agent Coordination System",
                "Monitoring and Analytics System",
                "Content Generation Pipeline",
                "Policy Analysis Engine"
            ]

            for system in supporting_systems:
                deployment_task = self._deploy_system(system)
                result = await deployment_task

                if result["status"] == "success":
                    deployment_results["systems_deployed"].append(system)

                logger.info(f"‚úÖ Deployed: {system}")

            deployment_results["deployment_status"] = "completed"
            deployment_results["end_time"] = datetime.now().isoformat()

            logger.info(f"üéâ Empire deployment completed! Deployed {len(deployment_results['systems_deployed'])} systems")

            return deployment_results

        except Exception as e:
            logger.error(f"‚ùå Empire deployment failed: {e}")
            deployment_results["deployment_status"] = "failed"
            deployment_results["error"] = str(e)
            return deployment_results

    async def _deploy_system(self, system_name: str) -> Dict[str, Any]:
        """Deploy a single system using agent swarm"""
        try:
            # Use agent swarm to deploy system
            deployment_agents = [agent for agent in self.orchestrator.agents.values() if "major" in agent.name.lower()]

            if not deployment_agents:
                deployment_agents = list(self.orchestrator.agents.values())[:3]  # Fallback to first 3 agents

            # Coordinate deployment across multiple agents
            deployment_tasks = []
            for agent in deployment_agents:
                task = agent.generate_response(
                    f"Deploy system: {system_name} for LivHana empire E2E deployment",
                    context="Empire-scale deployment with maximum speed and efficiency"
                )
                deployment_tasks.append(task)

            results = await asyncio.gather(*deployment_tasks, return_exceptions=True)

            successful_deployments = sum(1 for r in results if not isinstance(r, Exception) and not r.requires_correction)

            return {
                "status": "success" if successful_deployments > 0 else "partial",
                "system_name": system_name,
                "successful_deployments": successful_deployments,
                "total_attempts": len(deployment_tasks)
            }

        except Exception as e:
            return {
                "status": "failed",
                "system_name": system_name,
                "error": str(e)
            }

# Global scaler instance
agent_scaler = AgentSwarmScaler()

async def main():
    """Main scaling function"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    logger.info("üöÄ Starting LivHana Empire Agent Swarm Scaler...")

    # Optimize performance first
    optimization_results = await agent_scaler.optimize_swarm_performance()
    logger.info(f"‚ö° Performance optimization: {optimization_results}")

    # Scale the agent swarm
    scaling_results = await agent_scaler.scale_agent_swarm()
    logger.info(f"üìà Agent scaling results: {scaling_results}")

    # Deploy empire systems
    deployment_results = await agent_scaler.deploy_empire_systems()
    logger.info(f"üèóÔ∏è Empire deployment results: {deployment_results}")

    logger.info("üéâ LivHana Empire deployment and scaling completed!")

if __name__ == "__main__":
    asyncio.run(main())
